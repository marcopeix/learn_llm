{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with NIM\n",
    "\n",
    "Now that we've deployed NIM, lets learn how we can interact with it and integrate it with other applications by sending requests. The main way of interacting with NIM is through the REST API. In this notebook, we'll go over three ways of calling the REST API -- through `curl` commands, through the Python `requests` library, and through the Python `OpenAI` library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying that NIM is Ready\n",
    "\n",
    "You can check to see if the NIM is deployed and ready for inference by running the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"health.response\",\"message\":\"Service is ready.\"}"
     ]
    }
   ],
   "source": [
    "!curl localhost:8000/v1/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it responds with\n",
    "\n",
    "```json\n",
    "{\"object\":\"health.response\",\"message\":\"Service is ready.\"}\n",
    "```\n",
    "\n",
    "NIM is ready to start receiving requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the available models\n",
    "\n",
    "In order to send requests to NIM, we need to know what the name of our model is. We can obtain that by sending a request to the `v1/models` endpoint via the `requests` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'list', 'data': [{'id': 'meta/llama3-8b-instruct', 'object': 'model', 'created': 1720560692, 'owned_by': 'system', 'root': 'meta/llama3-8b-instruct', 'parent': None, 'permission': [{'id': 'modelperm-fb81edfbff96473eb6ca2f78d7230092', 'object': 'model_permission', 'created': 1720560692, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}]}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# List all models\n",
    "models = requests.get('http://localhost:8000/v1/models').json()\n",
    "print(models)\n",
    "\n",
    "# Get the name of our model\n",
    "model_name = models['data'][0]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Requests\n",
    "\n",
    "There are two different APIs for sending requests to NIM. The first is the Chat Completions API, and the second is the plain Completions API, both of which follow the API Specification from OpenAI. Let's see some examples of sending requests -- first using the python `requests` library, and then using the python `OpenAI` client library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completions API <a name=\"openai-chat\"></a>\n",
    "\n",
    "\n",
    "The OpenAI Chat API supports the following parameters:\n",
    "- messages\n",
    "- model\n",
    "- frequency_penalty\n",
    "- max_tokens\n",
    "- n = 1\n",
    "- stop\n",
    "- stream\n",
    "- temperature\n",
    "- top_p\n",
    "- logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the deep learning model go to therapy?\n",
      "\n",
      "Because it was feeling a little \"overfit\" and was struggling to \"generalize\" its thoughts!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "endpoint = 'http://0.0.0.0:8000/v1/chat/completions'\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a joke about deep learning.\"}\n",
    "]\n",
    "\n",
    "data = {\n",
    "    'model': model_name,\n",
    "    'messages': messages,\n",
    "    'max_tokens': 100,\n",
    "    'temperature': 1,\n",
    "    'n': 1,\n",
    "    'stream': False,\n",
    "    'stop': 'string',\n",
    "    'frequency_penalty': 0.0\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint, headers=headers, json=data)\n",
    "print(response.json()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the article \"SOFTS: The Latest Innovation in Time Series Forecasting\" by Marco Rosa, here is a summary:\n",
      "\n",
      "**What is Softs?**\n",
      "\n",
      "Softs is a novel method for time series forecasting that combines various techniques to improve predictive accuracy. The term \"SOFTS\" stands for Scalable Online Forest of Time Series Sub-models.\n",
      "\n",
      "**Challenges in Time Series Forecasting**\n",
      "\n",
      "Traditional time series forecasting methods often struggle with non-stationarity, non-normality, and\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"not-used\")\n",
    "blog_url = \"https://www.datasciencewithmarco.com/blog/softs-the-latest-innovation-in-time-series-forecasting\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Summarize the following blog article at this URL: {blog_url}\"}\n",
    "]\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"meta/llama3-8b-instruct\",\n",
    "    messages=messages,\n",
    "    max_tokens=100,\n",
    "    stream=False\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message\n",
    "print(assistant_message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform streaming inference with the OpenAI library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A question that asked in the minds of many AI enthusiasts!\n",
      "\n",
      "ChatGPT, Claude, and LLaMA are all highly advanced language models developed by Meta AI (formerly Facebook AI). While they share many similarities, each model has its unique strengths and is suited for specific tasks. Here's a brief rundown:\n",
      "\n",
      "1. **ChatGPT**: ChatGPT is a transformer-based model designed for conversational dialogue tasks. It's trained on a massive dataset of text from the internet and can generate human-like responses to user input. ChatGPT excels at tasks like chatbot conversations, text classification, and language translation.\n",
      "2. **Claude**: Claude is a large-scale language model similar to ChatGPT. However, it's specifically designed for more complex conversational tasks, such as storytelling, debate, and creative writing. Claude has been trained on a vast corpus of text and can generate longer, more cohesive responses.\n",
      "3. **LLaMA**: LLaMA is a text-generation model that's particularly good at producing longer, more detailed text. It's designed to be used as a content generation tool for applications like writing articles, creating chatbot responses, or even generating dialogue for movies. LLaMA is trained on a large corpus of text and can handle a wide range of topics.\n",
      "\n",
      "In summary:\n",
      "\n",
      "* ChatGPT is great for conversational dialogue and chatbot interactions.\n",
      "* Claude is ideal for complex conversational tasks, like storytelling and creative writing.\n",
      "* LLaMA is perfect for generating longer, more detailed text, making it suitable for content creation purposes.\n",
      "\n",
      "It's difficult to say which model is \"better\" since each has its unique strengths and is designed for specific use cases. If you're looking for a model that can have a natural conversation with users, ChatGPT or Claude might be a good choice. If you need a model that can generate longer, more detailed text, LLaMA could be the way to go."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"not-used\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who is better: ChatGPT, Claude, or Llama?\"}\n",
    "]\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"meta/llama3-8b-instruct\",\n",
    "    messages=messages,\n",
    "    max_tokens=1024,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "\n",
    "for chunk in chat_response:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
